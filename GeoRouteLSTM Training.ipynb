{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9d5c9-0a3f-4ec5-b534-99c7e3eb00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from models.rnn import CellType\n",
    "from models.geo_route_lstm import GeoRouteLSTM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset.geo_route import GeoRouteDataset, prepare_tensors\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d2e94-c1ba-472c-964d-c843c43c9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out if a CUDA device (GPU) is available\n",
    "if torch.cuda.device_count():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c0fa3-dbea-4346-ad25-2bd62d0e14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File that contains the data\n",
    "dataset_file = \"dataset_training.pkl.gz\"\n",
    "\n",
    "# The learning rate of the model\n",
    "lr = 1e-3\n",
    "\n",
    "# Cell type (LSTM | GRU | RNN)\n",
    "cell_type=CellType.LSTM\n",
    "\n",
    "# Number of epochs\n",
    "n_epochs = 100\n",
    "# Number of RNN layers\n",
    "num_layers=3\n",
    "# Embedding dimension\n",
    "embedding_dim=32\n",
    "# Hidden size of the RNN layers\n",
    "hidden_size=256\n",
    "# Batch size used for training\n",
    "batch_size=8192\n",
    "# Maximum sequence length\n",
    "max_length=39\n",
    "# True if bidirectional RNN layers should be used, False otherwise\n",
    "bidirectional=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d53200-309f-4083-90c6-86c8b3f5a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the dataset and a dataloader\n",
    "dataset = GeoRouteDataset(dataset_file)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c2cb8-25e9-491c-a01c-834b17a9348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataset.dest_cc).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e29426-bc17-4253-8b61-704a5a6f319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the network\n",
    "net = GeoRouteLSTM(device=device).to(device)\n",
    "\n",
    "# Create an optimizer and a learning rate scheduler for the network\n",
    "net_optimizer = torch.optim.AdamW(net.parameters(), lr=lr)\n",
    "net_scheduler = torch.optim.lr_scheduler.StepLR(net_optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06565fd6-4153-414e-8519-349a64888d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    # Get a batch of training data\n",
    "    for src_as, dest_as, src_cc, dest_cc, lat, long, asn, ip_source, geo_cc, labels in dataloader:\n",
    "        src_as, dest_as, src_cc, dest_cc, lat, long, asn, ip_source, geo_cc, labels = prepare_tensors(\n",
    "            src_as, dest_as, src_cc, dest_cc, lat, long, asn, ip_source, geo_cc, labels, device=device\n",
    "        )\n",
    "\n",
    "        # Turn labels into torch.long\n",
    "        labels = labels.to(torch.long).to(device)\n",
    "        \n",
    "        # Create masks for positive and negative labels\n",
    "        mask_class_0 = labels.squeeze() == 0\n",
    "        mask_class_1 = labels.squeeze() == 1\n",
    "\n",
    "        # Set gradients of all model parameters to zero\n",
    "        net_optimizer.zero_grad()\n",
    "\n",
    "        # Initialize loss\n",
    "        loss = 0\n",
    "        \n",
    "        # Get logits for each of the two classes\n",
    "        logits = net(\n",
    "            lat=lat,\n",
    "            long=long,\n",
    "            asn=asn,\n",
    "            ip_source=ip_source,\n",
    "            geo_cc=geo_cc,\n",
    "            src_as=src_as,\n",
    "            dest_as=dest_as,\n",
    "            src_cc=src_cc,\n",
    "            dest_cc=dest_cc,\n",
    "        )\n",
    "        \n",
    "        # Get the most likely class for each input\n",
    "        topv, topi = logits.topk(1)\n",
    "        \n",
    "        # Compute loss for positive and for negative samples\n",
    "        loss_class_0 = criterion(logits[mask_class_0].squeeze(), labels[mask_class_0].squeeze())\n",
    "        loss_class_1 = criterion(logits[mask_class_1].squeeze(), labels[mask_class_1].squeeze())\n",
    "        \n",
    "        # Compute the loss by putting equal weight on positive and negative samples (similar to focal loss)\n",
    "        loss = 0.5 * loss_class_0 + 0.5 * loss_class_1\n",
    "        \n",
    "        # Get number of positive and neagtive samples\n",
    "        n_class_0 = mask_class_0.sum().item()\n",
    "        n_class_1 = mask_class_1.sum().item()\n",
    "        \n",
    "        # Compute total accuracy and accuracies for both positive and negative samples\n",
    "        matchings = labels.squeeze() == topi.squeeze()\n",
    "        accuracy_total = matchings.sum().item() / batch_size\n",
    "        accuracy_class_0 = matchings[mask_class_0].sum().item() / n_class_0 if n_class_0 > 0 else 0.0\n",
    "        accuracy_class_1 = matchings[mask_class_1].sum().item() / n_class_1 if n_class_1 > 0 else 0.0\n",
    "        \n",
    "        print(f\"LOSS after epoch {epoch}\", loss.item() / (labels.size(1)), \"AccAll\", round(accuracy_total, 3), \"Acc0\", round(accuracy_class_0, 3), \"Acc1\", round(accuracy_class_1, 3))\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights of network\n",
    "        net_optimizer.step()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    net_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2442b81-7fd8-4019-8ed0-fc22472bc705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "filename = f\"{cell_type}_{num_layers}layers_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'net_state_dict': net.state_dict(),\n",
    "    'net_optimizer_state_dict': net_optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    \"lr\": lr,\n",
    "    \"cell_type\": cell_type,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"max_length\": max_length,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"bidirectional\": bidirectional,\n",
    "}, filename + \".pt\")\n",
    "    \n",
    "print(str(datetime.now()), \"Saved model: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0f243-bcb6-4336-8eed-1ac362d16de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
